# Masters_project
Repository for MRes. in AI and ML final project

## Abstract
Sparse Gaussan Process Regression (SGPR), proposed by Titsias, is a scalable Gaussian process method that shows excellent performance at regression tasks. It uses $M$ _inducing variables_ to summarise a dataset of $N$ points (where $M\leq N$) and makes an improvement to the computational cost of full Gaussian processes, which is impractically high when modelling large datasets: the time for computation scales as $\mathcal{O}(N^3)$ and thus, to learn from as few as 1000 datapoints, exact GPs are an unaffordable solution. For SGPR however, complexity scales as $\mathcal{O}(NM^2)$. This allows us to make use of the perks of GPs (uncertainty estimates, automatic regularisation, flexible priors, etc.) for a much broader range of problems.

Despite having all these theoretical benefits, SGPR has its practical problems. As well as training the model hyperparameters $\boldsymbol\theta$, as we do in the full Gaussian process model, we are additionally required to train the $M$ inducing variables. A joint optimisation over this large number of hyperparameters (as is the currently accepted method) is cumbersome. Furthermore, for many datasets, SGPR is unable to sufficiently summarise the dataset with the popular squared exponential (SE) kernel until $M=N$, thereby rendering the sparse method obsolete for that regression task.

This paper discusses experiments that aim to addresses these problems. Specifically, we verify that harnessing _greedy variance_ selection, a method for selecting the inducing points proposed by Burt et al., allows us to obtain an optimal solution for the SGPR model much faster and more consistently than a joint optimisation over all the model parameters. We additionally explore sparsity in SGPR. It is found that a good choice of kernel can allow us to use fewer inducing variables to ideally summarise the dataset than may be required for other kernels. Experiments are demonstrated on both toy and real-world datasets.
